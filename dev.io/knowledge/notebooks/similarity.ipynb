{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99462abe-1eb2-4988-8cc2-5c132a34027d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Prayson W. Daniel\n",
      "\n",
      "Last updated: 2024-11-14T19:38:08.956407+01:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.11.10\n",
      "IPython version      : 8.29.0\n",
      "\n",
      "ollama: 0.3.3\n",
      "torch : 2.5.1\n",
      "\n",
      "Compiler    : Clang 15.0.0 (clang-1500.3.9.4)\n",
      "OS          : Darwin\n",
      "Release     : 23.5.0\n",
      "Machine     : arm64\n",
      "Processor   : arm\n",
      "CPU cores   : 16\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout\n",
    "%reload_ext watermark\n",
    "%watermark -uniz --author \"Prayson W. Daniel\" -vm -p ollama,torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72497d6d-5c16-4082-be36-9f237322a2a0",
   "metadata": {},
   "source": [
    "# Using BM25s, Embedding + Cosine, LLM (via Ollama)\n",
    "```sh\n",
    "ollama pull mxbai-embed-large\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47ffe163-e0f8-4011-bd77-fd31ab7762f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bm25s\n",
    "import ollama\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2d83072-32d8-40f6-b6b1-b8e5d54bd155",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_cases = {\n",
    "    \"case1\": \"I like ice cream\",\n",
    "    \"case2\": \"I don't like ice cream\",\n",
    "    \"case3\": \"I hate ice cream\",\n",
    "    \"case4\": \"I enjoy ice cream\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b88f22e-d477-4227-a2c5-bcd7dabf22c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Matching 25 (= TF-IDF with document length normalization) does not capture negation.\n",
    "# ü§∑üèæ‚Äç‚ôÇÔ∏è Removing stopwords and applying lemmatization aims to improve matching accuracy.\n",
    "# Case1 and Case4 are incorrectly far apart with a score of 0.08731071 (Fails to capture semantic meaning).\n",
    "\n",
    "corpus = [case for case in tests_cases.values()]\n",
    "retriever = bm25s.BM25(corpus=corpus)\n",
    "retriever.index(bm25s.tokenize(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fa13157-6611-4138-8472-cc99c4260b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results(documents=array([['I like ice cream', \"I don't like ice cream\",\n",
       "        'I enjoy ice cream', 'I hate ice cream']], dtype='<U22'), scores=array([[0.37451115, 0.32753414, 0.08731071, 0.08731071]], dtype=float32))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = tests_cases.get(\"case1\")\n",
    "retriever.retrieve(bm25s.tokenize(query), k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1c288e3-337e-4ff6-bd8a-17e11b5d6e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding with Cosine Similarity\n",
    "# Case1 and Case4 are correctly identified as similar with a score of 0.9540 (captures semantic meaning).\n",
    "# However, Case1 and Case2 are also closely matched, even though I would prefer them to be further apart due to negation.\n",
    "\n",
    "\n",
    "results = {\n",
    "    case: torch.Tensor(\n",
    "        ollama.embeddings(\n",
    "            model=\"nomic-embed-text\",\n",
    "            prompt=blob,\n",
    "        ).get(\"embedding\")\n",
    "    ).reshape(1, -1)\n",
    "    for case, blob in tests_cases.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6328c4c-a8d4-4c97-9ab8-95893f5e6fe9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.0000]), tensor([0.8803]), tensor([0.8436]), tensor([0.9540]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    F.cosine_similarity(results[\"case1\"], results[\"case1\"]),\n",
    "    F.cosine_similarity(results[\"case1\"], results[\"case2\"]),\n",
    "    F.cosine_similarity(results[\"case1\"], results[\"case3\"]),\n",
    "    F.cosine_similarity(results[\"case1\"], results[\"case4\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff25ce2f-64a1-4a03-895e-a522fbc3d7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power of LLM\n",
    "# Case1 and Case4 are correctly identified as similar.\n",
    "# Bravo ü§òüèæ Case1 and Case2 are considered dissimilar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa818157-59d0-44c5-84d4-0c235011b9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ollama.Client(host=\"http://localhost:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "220a1946-11af-487e-97ea-8be2c916d011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -- case -- \n",
      "\n",
      "- case1: I like ice cream\n",
      "  case2: I don't like ice cream\n",
      "\n",
      "These two statements are contradictory, making them highly dissimilar. The similarity lies in the fact that they both express a preference or lack thereof for ice cream, but the tone and intention behind each statement are opposite, with one being positive and the other negative.\n",
      "\n",
      " -- case -- \n",
      "\n",
      "- case1: I like ice cream\n",
      "  case3: I hate ice cream\n",
      "\n",
      "These two statements are contradictory, as they express opposite emotions towards the same thing (ice cream). The similarity lies in their extreme nature, with one statement being an enthusiastic endorsement and the other a strong rejection, highlighting the intensity of the speaker's feelings.\n",
      "\n",
      " -- case -- \n",
      "\n",
      "- case1: I like ice cream\n",
      "  case4: I enjoy ice cream\n",
      "\n",
      "The two sentences are very similar, as they both express a positive sentiment towards ice cream. The only difference is that the first sentence uses \"like\" to describe the preference, while the second sentence uses \"enjoy\", which implies a stronger affection or appreciation for ice cream.\n"
     ]
    }
   ],
   "source": [
    "blob1 = tests_cases.pop(\"case1\")\n",
    "for case, blob in tests_cases.items():\n",
    "    print(\"\\n -- case -- \\n\")\n",
    "    print(f\"- case1: {blob1}\")\n",
    "    print(f\"  {case}: {blob}\\n\")\n",
    "\n",
    "    prompt = f\"How similiar are: {blob1} and {blob}. Give a short reasons why. ca. 2 sentence\"\n",
    "\n",
    "    output = client.generate(\n",
    "        model=\"llama3.2\",\n",
    "        prompt=prompt,\n",
    "        options={\n",
    "            \"seed\": 42,\n",
    "            \"temperature\": 0.0,\n",
    "            \"num_ctx\": 2048,  # must be set for reproducibility\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(output[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daf4009-603b-487f-8b3f-9d0c88242d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knowledge",
   "language": "python",
   "name": "knowledge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
