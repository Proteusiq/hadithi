
| Title & Link                                                                                     | Year | Summary                                                                                      | Phase                        | Code Repo                                                         |
|--------------------------------------------------------------------------------------------------|------|----------------------------------------------------------------------------------------------|------------------------------|-------------------------------------------------------------------|
| [Attention Is All You Need](https://arxiv.org/abs/1706.03762)                                    | 2017 | Introduced the Transformer architecture                                                      | Foundations                  | –                                                                 |
| [ULMFit](https://arxiv.org/abs/1801.06146)                                                       | 2017 | Early transfer learning success for NLP                                                      | Foundations                  | –                                                                 |
| [GPT-1](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) | 2018 | Unsupervised pre-training + supervised fine-tuning                                           | Foundations                  | –                                                                 |
| [BERT](https://arxiv.org/abs/1810.04805)                                                         | 2018 | Bidirectional encoder, key for NLP                                                          | Foundations                  | –                                                                 |
| [T5](https://arxiv.org/abs/1910.10683)                                                           | 2019 | Unified NLP tasks with text-to-text framework                                                | Foundations                  | –                                                                 |
| [GPT-2](https://openai.com/research/language-unsupervised)                                      | 2019 | Emergent capabilities from scale                                                              | Scaling (2019–2020)          | [openai/gpt-2](https://github.com/openai/gpt-2)                   |
| [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)                     | 2020 | Quantified performance scaling                                                              | Scaling                      | –                                                                 |
| [GPT-3](https://arxiv.org/abs/2005.14165)                                                        | 2020 | Few-shot learning via prompting                                                              | Few-Shot (2020–2021)         | –                                                                 |
| [FLAN](https://arxiv.org/abs/2109.01652)                                                         | 2021 | Instruction tuning yields zero-shot capabilities                                              | Few-Shot                     | [google-research/FLAN](https://github.com/google-research/FLAN)  [oai_citation:0‡GitHub](https://github.com/sergio11/todo-list-firebase?utm_source=chatgpt.com) |
| [Switch Transformers](https://arxiv.org/abs/2101.03961)                                          | 2021 | Efficient Mixture-of-Experts for scaling                                                     | Few-Shot                     | –                                                                 |
| [InstructGPT](https://arxiv.org/abs/2203.02155)                                                  | 2022 | Aligns to human preferences (RLHF)                                                           | Human Alignment (2022–24)    | –                                                                 |
| [Chain-of-Thought Prompting](https://arxiv.org/abs/2201.11903)                                   | 2022 | Strategy to improve reasoning in LLMs                                                        | Human Alignment              | –                                                                 |
| [Tree of Thoughts](https://arxiv.org/abs/2305.10601)                                             | 2023 | Structured reasoning via deliberate exploration                                               | Human Alignment              | –                                                                 |
| [LLaMA](https://arxiv.org/abs/2302.13971)                                                        | 2023 | Influential open foundation model                                                             | Open Models (2023–2025)      | [meta-llama/llama-models](https://github.com/meta-llama/llama-models)  [oai_citation:1‡GitHub](https://github.com/meta-llama/llama?utm_source=chatgpt.com) |
| [LLaMA 2](https://arxiv.org/abs/2307.09288)                                                      | 2023 | Instruction-tuned chat model                                                                  | Open Models                  | –                                                                 |
| [GPT-4 Technical Report](https://openai.com/research/gpt-4)                                      | 2023 | Multimodal breakthrough                                                                      | Open Models                  | –                                                                 |
| [LLaMA 3](https://llama.meta.com/llama3/)                                                        | 2024 | SOTA open model with improved training/data                                                  | Open Models                  | [meta-llama/llama3](https://github.com/meta-llama/llama3)  [oai_citation:2‡GitHub](https://github.com/meta-llama/llama3?utm_source=chatgpt.com) |
| [DeepSeekMath](https://arxiv.org/abs/2405.10863)                                                 | 2024 | Advanced mathematical reasoning                                                               | Open Models                  | –                                                                 |
| [Claude 3 Model Card](https://www.anthropic.com/news/claude-3)                                  | 2024 | Claude series technical improvements                                                         | Open Models                  | –                                                                 |
| [Claude 4 Technical Report](https://www-cdn.anthropic.com/4263b940cabb546aa0e3283f35b686f4f3b2ff47.pdf) | 2025 | Latest model details via official PDF                                                         | Open Models                  | –                                                                 |
| — Additional Repos —                                                                             |      |                                                                                              |                              |                                                                   |
| FLAN extensions (e.g., Flan-Alpaca)                                                             | 2022 | Instruction tuning extensions                                                                | Few-Shot                     | [declare-lab/flan-alpaca](https://github.com/declare-lab/flan-alpaca)  [oai_citation:3‡GitHub](https://github.com/declare-lab/flan-alpaca?utm_source=chatgpt.com) |
| LLaMA adapters or implementations                                                                | —    | Model fine-tuning tools                                                                      | Open Models                  | [OpenGVLab/LLaMA-Adapter](https://github.com/OpenGVLab/LLaMA-Adapter)  [oai_citation:4‡arxiv.org](https://arxiv.org/abs/2303.16199?utm_source=chatgpt.com) |
| llama.cpp (lighter inference implementation)                                                     | 2023 | Client-side inference for LLaMA models                                                       | Open Models                  | [ggml‑org/llama.cpp](https://github.com/ggml-org/llama.cpp)  [oai_citation:5‡en.wikipedia.org](https://en.wikipedia.org/wiki/Llama.cpp?utm_source=chatgpt.com) [oai_citation:6‡zh.wikipedia.org](https://zh.wikipedia.org/wiki/Llama.cpp?utm_source=chatgpt.com) |
