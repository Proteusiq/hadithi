
# LLM Papers

| Title & Link                                                                                     | Year | Summary                                                                                     | Phase                        | Code Repo |
|--------------------------------------------------------------------------------------------------|------|---------------------------------------------------------------------------------------------|------------------------------|-----------|
| [Attention Is All You Need](https://arxiv.org/abs/1706.03762)                                    | 2017 | Introduced the Transformer architecture                                                     | Foundations (2017–2019)     | –         |
| [ULMFit](https://arxiv.org/abs/1801.06146)                                                       | 2017 | Early transfer learning success for NLP                                                     | Foundations                 | –         |
| [GPT-1](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) | 2018 | Unsupervised pre-training + supervised fine-tuning                                          | Foundations                 | –         |
| [BERT](https://arxiv.org/abs/1810.04805)                                                         | 2018 | Bidirectional encoder, key to many NLP tasks                                                | Foundations                 | –         |
| [T5](https://arxiv.org/abs/1910.10683)                                                           | 2019 | Unified NLP tasks as text-to-text transformations                                           | Foundations                 | –         |
| [GPT-2](https://openai.com/research/language-unsupervised)                                      | 2019 | Showed emergence of capabilities from scale                                                 | Scaling (2019–2020)        | [openai/gpt-2](https://github.com/openai/gpt-2) |
| [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)                     | 2020 | Quantified the impact of scaling model size, data, and compute                             | Scaling                     | –         |
| [GPT-3](https://arxiv.org/abs/2005.14165)                                                        | 2020 | Few-shot learning via prompting, emergent abilities                                         | Few-Shot (2020–2021)       | –         |
| [FLAN](https://arxiv.org/abs/2109.01652)                                                         | 2021 | Instruction tuning improves zero-shot performance                                           | Few-Shot                    | –         |
| [Switch Transformers](https://arxiv.org/abs/2101.03961)                                          | 2021 | Mixture-of-Experts (MoE) model for efficient scaling                                        | Few-Shot                    | –         |
| [InstructGPT](https://arxiv.org/abs/2203.02155)                                                  | 2022 | Aligns LLMs to human preferences via RLHF                                                  | Human Alignment (2022–24)  | –         |
| [Chain-of-Thought Prompting](https://arxiv.org/abs/2201.11903)                                   | 2022 | Prompting strategy to improve reasoning in LLMs                                             | Human Alignment             | –         |
| [Tree of Thoughts](https://arxiv.org/abs/2305.10601)                                             | 2023 | Structured reasoning via deliberate thought exploration                                     | Human Alignment             | –         |
| [LLaMA](https://arxiv.org/abs/2302.13971)                                                        | 2023 | Open foundation model — strong performance on limited budget                                | Open Models (2023–2025)    | –         |
| [LLaMA 2](https://arxiv.org/abs/2307.09288)                                                      | 2023 | Fine-tuned for chat, improved benchmarks                                                    | Open Models                 | –         |
| [GPT-4 Technical Report](https://openai.com/research/gpt-4)                                      | 2023 | Multimodal model with strong general performance                                            | Open Models                 | –         |
| [LLaMA 3](https://llama.meta.com/llama3/)                                                        | 2024 | State-of-the-art open model, improved training/data                                         | Open Models                 | –         |
| [DeepSeekMath](https://arxiv.org/abs/2405.10863)                                                 | 2024 | Specialized model for mathematical reasoning                                                | Open Models                 | –         |
| [Claude 3 Model Card](https://www.anthropic.com/news/claude-3)                                  | 2024 | Claude series technical improvements                                                        | Open Models                 | –         |
| [Claude 4 Model Card](https://www-cdn.anthropic.com/4263b940cabb546aa0e3283f35b686f4f3b2ff47.pdf             )                                                                          | 2025 | Model details TBD — known by SHA `4263b940...`                                              | Open Models                 | –         |
